{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5pGGECa76IcTZ8QOSagFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Districts/5/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model District 5\n",
        "\n",
        "Predictors:\n",
        "*   day_of_week\n",
        "*   max\n",
        "*   sndp\n",
        "*   snow_ice_pellets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/5/generic_set_district5.0.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generic.corr()['crime_count']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Ytyqvss1V9",
        "outputId": "5fb504ff-f1d5-4a6d-dcfe-7f98e1f268e7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                   0.036791\n",
              "day_of_week            -0.026737\n",
              "district                     NaN\n",
              "crime_count             1.000000\n",
              "mo                      0.040733\n",
              "temp                    0.440717\n",
              "dewp                    0.405424\n",
              "slp                    -0.173363\n",
              "stp                     0.083265\n",
              "visib                   0.172255\n",
              "wdsp                   -0.147344\n",
              "mxpsd                  -0.084479\n",
              "gust                   -0.093793\n",
              "max                     0.443498\n",
              "min                     0.420053\n",
              "prcp                    0.010261\n",
              "sndp                   -0.233108\n",
              "fog                    -0.087639\n",
              "rain_drizzle            0.026359\n",
              "snow_ice_pellets       -0.265183\n",
              "hail                         NaN\n",
              "thunder                 0.097296\n",
              "tornado_funnel_cloud         NaN\n",
              "Name: crime_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'max', 'sndp', 'snow_ice_pellets', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "95699697-9811-412a-e827-f084f5da0721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 4\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "ad9362ea-279a-4c20-d22c-1e4f2a161244"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "92ac90a2-fa78-42bd-9368-730a4494cbed"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0388\n",
            "Epoch 2/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0210\n",
            "Epoch 3/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0189\n",
            "Epoch 4/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 5/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 6/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 7/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 8/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 9/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 10/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 11/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 12/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 13/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 14/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 15/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 16/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 17/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 18/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 19/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 20/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 21/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 22/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 23/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 24/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 25/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 26/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 27/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 28/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 29/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 30/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 31/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 32/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 33/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0190\n",
            "Epoch 34/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 35/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 36/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 37/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0190\n",
            "Epoch 38/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 39/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 40/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 41/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0190\n",
            "Epoch 42/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 43/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 44/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 45/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 46/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 47/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 48/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 49/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 50/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 51/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 52/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 53/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 54/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 55/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 56/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 57/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 58/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 59/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 60/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 61/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 62/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 63/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 64/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 65/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 66/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 67/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 68/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 69/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 70/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 71/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0189\n",
            "Epoch 72/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 73/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 74/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 75/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 76/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 77/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 78/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 79/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 80/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 81/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 82/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 83/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 84/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 85/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0191\n",
            "Epoch 86/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 87/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 88/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 89/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 90/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 91/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 92/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 93/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 94/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 95/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 96/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 97/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 98/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0191\n",
            "Epoch 99/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 100/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 101/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0184\n",
            "Epoch 102/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 103/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 104/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 105/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 106/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 107/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 108/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 109/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 110/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 111/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 112/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 113/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 114/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 115/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 116/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 117/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 118/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 119/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 120/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 121/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 122/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 123/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 124/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 125/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0187\n",
            "Epoch 126/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0185\n",
            "Epoch 127/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0183\n",
            "Epoch 128/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0190\n",
            "Epoch 129/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0190\n",
            "Epoch 130/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 131/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0189\n",
            "Epoch 132/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0187\n",
            "Epoch 133/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0188\n",
            "Epoch 134/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0185\n",
            "Epoch 135/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0186\n",
            "Epoch 136/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 137/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 138/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0186\n",
            "Epoch 139/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 140/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 141/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 142/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 143/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 144/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 145/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 146/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0188\n",
            "Epoch 147/250\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0185\n",
            "Epoch 148/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0184\n",
            "Epoch 149/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0186\n",
            "Epoch 150/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 151/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 152/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 153/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 154/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 155/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 156/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 157/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 158/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 159/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 160/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 161/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 162/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 163/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 164/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 165/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 166/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 167/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 168/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 169/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 170/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 171/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 172/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 173/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 174/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 175/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 176/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 177/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 178/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 179/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 180/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 181/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 182/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 183/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 184/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 185/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 186/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 187/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 188/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 189/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 190/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 191/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 192/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 193/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 194/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 195/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 196/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 197/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 198/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 199/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 200/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 201/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 202/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 203/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 204/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 205/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 206/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 207/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 208/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 209/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 210/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 211/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 212/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 213/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 214/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 215/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 216/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 217/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 218/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0189\n",
            "Epoch 219/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0191\n",
            "Epoch 220/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 221/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 222/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 223/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 224/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 225/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 226/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 227/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 228/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 229/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 230/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 231/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 232/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 233/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 234/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0191\n",
            "Epoch 235/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 236/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 237/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 238/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 239/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 240/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 241/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 242/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0187\n",
            "Epoch 243/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0186\n",
            "Epoch 244/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 245/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 246/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 247/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 248/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 249/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0187\n",
            "Epoch 250/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "15a6bbf6-b566-4c22-b7f0-06eac89b829e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efe23192610>]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "178419f1-5cbb-4538-bf65-e2b39258e744"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.14960251251949203\n",
            "Using the training data mean of 0.4649384415385793 would have has resulted in a RMSE of 0.1613997297420388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/5/test_set_district5.0.csv')\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "69247fbd-9180-46cf-8deb-643a4ba974b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   predicted  actual  error_squared\n",
            "0  29.589905      28       2.527797\n",
            "1  30.639542      33       5.571764\n",
            "2  35.671463      25     113.880123\n",
            "3  28.021616      22      36.259859\n",
            "4  31.689178      36      18.583182\n",
            "The RMSE on the 5 test values is 5.946809650882735.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}