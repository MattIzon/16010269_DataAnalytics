{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4jxMdpP9XZsEIV5mwgDrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Districts/8/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model District 8\n",
        "\n",
        "Predictors:\n",
        "*   day_of_week\n",
        "*   max\n",
        "*   snow_ice_pellets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "district = 8\n",
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/{0}/generic_set_district{0}.0.csv'.format(district))"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generic.corr()['crime_count']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Ytyqvss1V9",
        "outputId": "984cf743-2342-4610-a2e5-cf8c12263205"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                   0.068219\n",
              "day_of_week            -0.045871\n",
              "district                     NaN\n",
              "crime_count             1.000000\n",
              "mo                      0.070986\n",
              "temp                    0.360480\n",
              "dewp                    0.330703\n",
              "slp                    -0.137463\n",
              "stp                     0.094699\n",
              "visib                   0.163386\n",
              "wdsp                   -0.085484\n",
              "mxpsd                  -0.059145\n",
              "gust                   -0.050382\n",
              "max                     0.362012\n",
              "min                     0.349739\n",
              "prcp                   -0.052070\n",
              "sndp                   -0.178224\n",
              "fog                    -0.086832\n",
              "rain_drizzle            0.015380\n",
              "snow_ice_pellets       -0.211571\n",
              "hail                         NaN\n",
              "thunder                 0.059204\n",
              "tornado_funnel_cloud         NaN\n",
              "Name: crime_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'max', 'snow_ice_pellets', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "bf764c9d-e095-4149-b25b-0dfd6b4bcdcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 3\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "fde85baf-47e9-4005-dad0-fc6de05d6272"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4\n",
            "Trainable params: 4\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "1264d622-09b8-4fb3-cf12-a6dbeaacb79e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2387\n",
            "Epoch 2/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1149\n",
            "Epoch 3/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0617\n",
            "Epoch 4/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0401\n",
            "Epoch 5/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0325\n",
            "Epoch 6/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 7/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 8/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 9/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 10/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 11/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 12/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 13/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 14/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 15/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 16/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 17/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 18/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 19/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 20/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 21/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 22/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 23/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 24/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 25/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 26/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 27/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 28/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 29/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0309\n",
            "Epoch 30/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 31/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 32/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 33/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 34/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 35/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 36/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 37/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 38/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0307\n",
            "Epoch 39/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0320\n",
            "Epoch 40/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 41/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 42/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0303\n",
            "Epoch 43/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 44/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 45/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 46/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 47/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 48/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 49/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0303\n",
            "Epoch 50/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 51/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 52/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 53/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 54/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 55/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 56/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 57/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 58/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 59/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 60/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 61/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 62/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 63/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 64/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 65/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 66/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 67/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 68/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 69/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 70/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 71/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 72/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 73/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 74/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 75/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 76/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 77/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 78/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 79/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 80/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 81/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 82/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0307\n",
            "Epoch 83/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 84/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 85/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0305\n",
            "Epoch 86/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 87/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 88/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 89/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 90/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 91/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 92/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 93/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 94/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 95/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 96/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 97/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0304\n",
            "Epoch 98/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 99/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 100/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 101/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 102/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 103/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 104/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 105/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 106/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 107/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 108/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 109/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 110/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 111/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 112/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0308\n",
            "Epoch 113/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 114/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 115/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 116/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 117/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 118/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 119/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 120/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 121/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 122/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 123/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 124/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 125/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 126/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 127/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 128/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 129/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 130/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 131/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0306\n",
            "Epoch 132/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 133/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 134/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0314\n",
            "Epoch 135/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 136/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0305\n",
            "Epoch 137/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 138/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 139/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 140/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 141/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 142/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 143/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 144/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 145/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0299\n",
            "Epoch 146/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 147/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 148/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 149/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 150/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0313\n",
            "Epoch 151/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0308\n",
            "Epoch 152/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 153/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0306\n",
            "Epoch 154/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 155/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 156/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 157/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 158/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 159/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 160/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 161/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 162/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 163/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 164/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 165/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 166/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 167/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 168/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0312\n",
            "Epoch 169/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 170/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 171/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 172/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 173/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 174/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 175/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 176/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 177/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 178/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 179/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 180/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 181/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 182/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 183/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 184/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 185/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0299\n",
            "Epoch 186/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 187/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 188/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0305\n",
            "Epoch 189/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0308\n",
            "Epoch 190/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301\n",
            "Epoch 191/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 192/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 193/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 194/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 195/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 196/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 197/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 198/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 199/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 200/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 201/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 202/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 203/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 204/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 205/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 206/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 207/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 208/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 209/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 210/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 211/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0299\n",
            "Epoch 212/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0311\n",
            "Epoch 213/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0310\n",
            "Epoch 214/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 215/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0311\n",
            "Epoch 216/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0303\n",
            "Epoch 217/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 218/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 219/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 220/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 221/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 222/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 223/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0304\n",
            "Epoch 224/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 225/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 226/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 227/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 228/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 229/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 230/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 231/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 232/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 233/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 234/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 235/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0302\n",
            "Epoch 236/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 237/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 238/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 239/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 240/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0301\n",
            "Epoch 241/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0306\n",
            "Epoch 242/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 243/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 244/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0309\n",
            "Epoch 245/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0300\n",
            "Epoch 246/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n",
            "Epoch 247/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 248/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0310\n",
            "Epoch 249/250\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 250/250\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "cc85db3a-ffa6-4df2-912a-8d8738f7c182"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efe241a3e90>]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "487ff99e-3b24-40b2-a510-daa622b202f0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.16436464352687694\n",
            "Using the training data mean of 0.6680131132185927 would have has resulted in a RMSE of 0.17658153613859898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/{0}/test_set_district{0}.0.csv'.format(district))\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "3d7c0ac7-c929-4e41-f4bc-a96910daa4e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   predicted  actual  error_squared\n",
            "0  42.767036      52      85.247616\n",
            "1  43.744198      45       1.577039\n",
            "2  41.793358      45      10.282554\n",
            "3  47.079117      24     532.645633\n",
            "4  39.573647      39       0.329070\n",
            "The RMSE on the 5 test values is 11.225701872777025.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}