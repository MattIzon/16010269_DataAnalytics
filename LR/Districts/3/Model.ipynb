{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP08LgBOJx1aqzmr22PVaLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Districts/3/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model District 3\n",
        "\n",
        "Predictors:\n",
        "*   day_of_week\n",
        "*   temp\n",
        "*   slp\n",
        "*   sndp\n",
        "*   snow_ice_pellets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/3/generic_set_district3.0.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generic.corr()['crime_count']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Ytyqvss1V9",
        "outputId": "c2633206-5cea-4211-ec5b-927aa5777b93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                   0.074824\n",
              "day_of_week            -0.094217\n",
              "district                     NaN\n",
              "crime_count             1.000000\n",
              "mo                      0.080160\n",
              "temp                    0.431066\n",
              "dewp                    0.408046\n",
              "slp                    -0.205634\n",
              "stp                     0.103013\n",
              "visib                   0.151587\n",
              "wdsp                   -0.110601\n",
              "mxpsd                  -0.058583\n",
              "gust                   -0.039393\n",
              "max                     0.435124\n",
              "min                     0.419757\n",
              "prcp                    0.035975\n",
              "sndp                   -0.224454\n",
              "fog                    -0.090228\n",
              "rain_drizzle            0.052112\n",
              "snow_ice_pellets       -0.245709\n",
              "hail                         NaN\n",
              "thunder                 0.092982\n",
              "tornado_funnel_cloud         NaN\n",
              "Name: crime_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'temp', 'slp', 'sndp', 'snow_ice_pellets', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "b30fea6e-caef-46c9-fa01-a117fa7966da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 5\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "7fe2c682-c43c-46bb-b9dc-da38c2c86a3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6\n",
            "Trainable params: 6\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "8acf9f6a-a3c1-4007-84fe-5e118069ce5d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.1519\n",
            "Epoch 2/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0493\n",
            "Epoch 3/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0376\n",
            "Epoch 4/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0320\n",
            "Epoch 5/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0286\n",
            "Epoch 6/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0265\n",
            "Epoch 7/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0252\n",
            "Epoch 8/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0246\n",
            "Epoch 9/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 10/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 11/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 12/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 13/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 14/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 15/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 16/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 17/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 18/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 19/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 20/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 21/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 22/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 23/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 24/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 25/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 26/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 27/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 28/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 29/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 30/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 31/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 32/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 33/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 34/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 35/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 36/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 37/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 38/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 39/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 40/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 41/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 42/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 43/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 44/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 45/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 46/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 47/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 48/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 49/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 50/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 51/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 52/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 53/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 54/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 55/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0232\n",
            "Epoch 56/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 57/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 58/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 59/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 60/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 61/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 62/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 63/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 64/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 65/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 66/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 67/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 68/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 69/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0238\n",
            "Epoch 70/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 71/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 72/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 73/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 74/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 75/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 76/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 77/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 78/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 79/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 80/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 81/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 82/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 83/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 84/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 85/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 86/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0240\n",
            "Epoch 87/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 88/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 89/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 90/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 91/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 92/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 93/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 94/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 95/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 96/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 97/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 98/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 99/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 100/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 101/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 102/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 103/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 104/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 105/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 106/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 107/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0233\n",
            "Epoch 108/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 109/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 110/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 111/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 112/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 113/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 114/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 115/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0233\n",
            "Epoch 116/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 117/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0232\n",
            "Epoch 118/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0238\n",
            "Epoch 119/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 120/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 121/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0239\n",
            "Epoch 122/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 123/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 124/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 125/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0241\n",
            "Epoch 126/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 127/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 128/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 129/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 130/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 131/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 132/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 133/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 134/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 135/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 136/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 137/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 138/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 139/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 140/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 141/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 142/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 143/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 144/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 145/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 146/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 147/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0235\n",
            "Epoch 148/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0236\n",
            "Epoch 149/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 150/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 151/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 152/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 153/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 154/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 155/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 156/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 157/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 158/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 159/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0234\n",
            "Epoch 160/250\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0234\n",
            "Epoch 161/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 162/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 163/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0232\n",
            "Epoch 164/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 165/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 166/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 167/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 168/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 169/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 170/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 171/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 172/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 173/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 174/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 175/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 176/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 177/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0241\n",
            "Epoch 178/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 179/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 180/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 181/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 182/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 183/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 184/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 185/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 186/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 187/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 188/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 189/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 190/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 191/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 192/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 193/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 194/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 195/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0240\n",
            "Epoch 196/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 197/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 198/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 199/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 200/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 201/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 202/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0235\n",
            "Epoch 203/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 204/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 205/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 206/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 207/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 208/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 209/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 210/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 211/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0240\n",
            "Epoch 212/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 213/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 214/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 215/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 216/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 217/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 218/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 219/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 220/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 221/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 222/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 223/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 224/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0239\n",
            "Epoch 225/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 226/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0237\n",
            "Epoch 227/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 228/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 229/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 230/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 231/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 232/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 233/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 234/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 235/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0240\n",
            "Epoch 236/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n",
            "Epoch 237/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 238/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 239/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 240/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0235\n",
            "Epoch 241/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0231\n",
            "Epoch 242/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0238\n",
            "Epoch 243/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 244/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 245/250\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 246/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 247/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0234\n",
            "Epoch 248/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0232\n",
            "Epoch 249/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0231\n",
            "Epoch 250/250\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.0233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "23ac89c4-0781-4279-a8b5-d9b52ec86825"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efe282c97d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "df8a87b5-c627-48ad-cacd-b61ab7ae5187"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.15356545023231533\n",
            "Using the training data mean of 0.4602298850574712 would have has resulted in a RMSE of 0.17232003044133345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Districts/3/test_set_district3.0.csv')\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "a851e548-01a5-4bfb-8755-24a2c000b9f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   predicted  actual  error_squared\n",
            "0  34.995251      37       4.019020\n",
            "1  33.830399      32       3.350359\n",
            "2  28.361744      21      54.195274\n",
            "3  31.322826      23      69.269439\n",
            "4  38.536407      30      72.870253\n",
            "The RMSE on the 5 test values is 6.382857413899531.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}