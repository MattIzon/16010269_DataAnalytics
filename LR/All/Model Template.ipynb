{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQJ8qDtz9qHfMOXlw093Lh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Normalised/Model%20Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Test Model\n",
        "All models have been useless. I suspect either my method for producing models is incorrect or my method for evaluating models is incorrect.\n",
        "\n",
        "This model is operating on perfectly linear data. In theory the model should be perfect"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Normalised/generic_set.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'temp', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "01e0ef86-6798-4c81-dc18-ec7014a308aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 2\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "a08971c9-4dd2-4a1c-bf87-0276137ec5b1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "7e65dd19-c4d6-48d5-8b19-d019bbf89882"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0446\n",
            "Epoch 2/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0239\n",
            "Epoch 3/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0193\n",
            "Epoch 4/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 5/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 6/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 7/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 8/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 9/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 10/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 11/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 12/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 13/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 14/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 15/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 16/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 17/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 18/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 19/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 20/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 21/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 22/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 23/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 24/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 25/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 26/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 27/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 28/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 29/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 30/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0184\n",
            "Epoch 31/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 32/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 33/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 34/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 35/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 36/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 37/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 38/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 39/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 40/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 41/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 42/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 43/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 44/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 45/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 46/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 47/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 48/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 49/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 50/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 51/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 52/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 53/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 54/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 55/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 56/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 57/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 58/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 59/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 60/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 61/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 62/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 63/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 64/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 65/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 66/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 67/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 68/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 69/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 70/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 71/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 72/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 73/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 74/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 75/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 76/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 77/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 78/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 79/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 80/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 81/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 82/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 83/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 84/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 85/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 86/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 87/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 88/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 89/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 90/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 91/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 92/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 93/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 94/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 95/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 96/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 97/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 98/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 99/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 100/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 101/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 102/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 103/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 104/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 105/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 106/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 107/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 108/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 109/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 110/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 111/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 112/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 113/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 114/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 115/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 116/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 117/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 118/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 119/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 120/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 121/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 122/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 123/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 124/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 125/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 126/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 127/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 128/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 129/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 130/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 131/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 132/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 133/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 134/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 135/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 136/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 137/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 138/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 139/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 140/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 141/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 142/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 143/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 144/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 145/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 146/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 147/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 148/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 149/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 150/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 151/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 152/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 153/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 154/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 155/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 156/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 157/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 158/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 159/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 160/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 161/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 162/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 163/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 164/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 165/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 166/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 167/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0190\n",
            "Epoch 168/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 169/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 170/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 171/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 172/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 173/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 174/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 175/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 176/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 177/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 178/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 179/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 180/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 181/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 182/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 183/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 184/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 185/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 186/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 187/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 188/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 189/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 190/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 191/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 192/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0178\n",
            "Epoch 193/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 194/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 195/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 196/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 197/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 198/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 199/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 200/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 201/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 202/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 203/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 204/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 205/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 206/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 207/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 208/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 209/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 210/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 211/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 212/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 213/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 214/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 215/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 216/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 217/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 218/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 219/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 220/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 221/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0191\n",
            "Epoch 222/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 223/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 224/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 225/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 226/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 227/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 228/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 229/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 230/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 231/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 232/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 233/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 234/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 235/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 236/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 237/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 238/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 239/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 240/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 241/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 242/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 243/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 244/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 245/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 246/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 247/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 248/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 249/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 250/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "134774ec-5ddc-4857-a69d-c466e253e361"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f00c4f667d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "1aaffa2b-fe6a-48f0-e493-844096d5a6c5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.1307801336496294\n",
            "Using the training data mean of 0.5156161260806462 would have has resulted in a RMSE of 0.1805106219054207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Normalised/test_set.csv')\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "09275cc7-d1ef-41e8-a615-1838402a12cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    predicted  actual  error_squared\n",
            "0  769.732971     726    1912.572769\n",
            "1  638.622803     626     159.335149\n",
            "2  716.875793     732     228.741624\n",
            "3  705.839539     735     850.332511\n",
            "4  785.345154     794      74.906363\n",
            "The RMSE on the 5 test values is 25.40034808743797.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}