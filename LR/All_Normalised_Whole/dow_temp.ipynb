{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWaGEtGdYA7wWOS0L0UtV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Normalised/dow_temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "Predictors:\n",
        "*   day_of_week\n",
        "*   temp\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/All_Normalised_Whole/generic_set.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'temp', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "9e0372b5-d7b3-4d9c-82f4-5350e3b176a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 2\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "a705f0d2-8df6-4e81-9bde-2dfbcd3f5d0c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "cefa2b30-b3ef-49c2-bc87-961b78b99043"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.2881\n",
            "Epoch 2/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0268\n",
            "Epoch 3/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0246\n",
            "Epoch 4/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 5/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0225\n",
            "Epoch 6/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0216\n",
            "Epoch 7/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0208\n",
            "Epoch 8/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0202\n",
            "Epoch 9/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0196\n",
            "Epoch 10/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0193\n",
            "Epoch 11/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 12/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 13/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 14/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 15/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 16/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 17/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 18/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 19/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 20/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 21/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 22/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 23/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 24/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 25/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 26/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 27/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 28/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 29/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 30/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 31/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 32/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 33/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 34/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 35/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 36/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 37/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 38/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 39/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 40/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 41/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 42/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 43/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 44/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 45/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 46/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 47/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 48/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 49/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 50/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 51/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 52/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 53/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 54/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 55/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 56/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 57/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 58/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 59/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 60/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 61/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 62/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 63/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 64/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 65/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0182\n",
            "Epoch 66/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 67/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 68/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 69/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 70/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 71/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 72/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 73/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 74/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 75/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 76/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 77/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 78/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 79/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 80/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 81/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 82/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 83/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 84/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 85/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 86/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 87/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 88/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 89/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 90/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 91/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 92/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 93/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 94/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 95/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 96/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 97/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 98/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 99/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 100/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 101/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 102/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 103/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 104/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 105/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 106/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 107/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 108/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 109/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 110/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 111/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 112/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 113/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 114/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 115/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 116/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 117/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 118/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 119/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 120/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 121/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 122/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 123/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 124/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 125/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 126/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 127/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 128/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 129/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 130/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 131/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 132/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 133/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 134/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 135/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 136/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 137/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 138/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 139/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 140/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 141/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 142/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 143/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 144/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 145/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 146/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 147/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 148/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 149/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 150/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 151/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 152/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 153/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 154/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 155/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 156/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 157/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 158/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 159/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 160/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 161/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 162/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 163/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 164/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 165/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 166/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 167/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 168/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 169/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 170/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 171/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 172/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 173/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 174/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 175/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 176/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 177/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 178/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 179/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 180/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 181/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 182/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 183/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 184/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 185/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 186/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 187/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 188/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 189/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 190/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 191/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 192/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 193/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 194/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 195/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 196/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 197/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 198/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 199/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 200/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 201/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 202/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 203/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 204/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 205/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 206/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 207/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 208/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 209/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 210/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 211/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 212/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 213/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 214/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 215/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 216/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0182\n",
            "Epoch 217/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 218/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 219/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 220/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 221/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0178\n",
            "Epoch 222/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 223/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 224/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 225/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 226/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 227/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 228/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 229/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 230/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 231/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 232/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 233/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 234/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 235/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 236/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 237/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0178\n",
            "Epoch 238/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 239/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 240/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 241/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 242/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 243/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 244/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 245/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 246/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 247/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 248/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 249/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 250/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "e9wcugdS2zyM",
        "outputId": "9bb88731-a9e0-471b-fa26-6825fa4bf19d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcf95eebf90>]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "bba436d3-fbdf-4669-810d-2fcee333bfb2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.13320115116348052\n",
            "Using the training data mean of 0.5156161260806462 would have has resulted in a RMSE of 0.1805106219054207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Normalised/test_set.csv')\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "3c5fa373-d66c-48ac-db3f-386e79ed4e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    predicted  actual  error_squared\n",
            "0  779.016174     726    2810.714739\n",
            "1  644.285034     626     334.342475\n",
            "2  724.609802     732      54.615023\n",
            "3  713.807861     735     449.106741\n",
            "4  794.257446     794       0.066279\n",
            "The RMSE on the 5 test values is 27.014237938618543.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}
