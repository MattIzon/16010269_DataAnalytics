{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPsyMaME61uP1XQsMcwGCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/LR/Normalised/dow_temp_sip_wdsp_visib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "Predictors:\n",
        "*   day_of_week\n",
        "*   temp\n",
        "*   snow_ice_pellets\n",
        "*   wdsp\n",
        "*   visib\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/Normalised/generic_set.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generic.corr()['crime_count']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9z9nohDSPep",
        "outputId": "4ddd1b4d-68f0-4891-f7d0-3c59ba6645c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                   0.197341\n",
              "day_of_week             0.038719\n",
              "crime_count             1.000000\n",
              "mo                      0.199961\n",
              "temp                    0.715968\n",
              "dewp                    0.673428\n",
              "slp                    -0.272455\n",
              "stp                     0.161359\n",
              "visib                   0.216665\n",
              "wdsp                   -0.226882\n",
              "mxpsd                  -0.145715\n",
              "gust                   -0.148226\n",
              "max                     0.709525\n",
              "min                     0.697839\n",
              "prcp                    0.003773\n",
              "sndp                   -0.346634\n",
              "fog                    -0.092184\n",
              "rain_drizzle            0.005482\n",
              "snow_ice_pellets       -0.378491\n",
              "hail                         NaN\n",
              "thunder                 0.132925\n",
              "tornado_funnel_cloud         NaN\n",
              "Name: crime_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['day_of_week', 'temp', 'snow_ice_pellets', 'wdsp', 'visib', 'crime_count']\n",
        "data = generic[columns]\n",
        "scale_params = normalise(data, columns)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF",
        "outputId": "9daa5c8e-c135-4e20-d58d-fdada9ae2589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = 5\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,0:qty_predictors]\n",
        "train_targets = data.iloc[:train_size,qty_predictors]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,0:qty_predictors]\n",
        "eval_targets = data.iloc[train_size:,qty_predictors]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(qty_targets, input_shape=[qty_predictors]))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "8f6648e8-bf0b-4134-c769-95f582c6e24f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6\n",
            "Trainable params: 6\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "9bd70c61-3eb6-4265-ee0b-7a10013cf301"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.4026\n",
            "Epoch 2/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0910\n",
            "Epoch 3/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0616\n",
            "Epoch 4/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0490\n",
            "Epoch 5/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0408\n",
            "Epoch 6/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0348\n",
            "Epoch 7/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0305\n",
            "Epoch 8/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0273\n",
            "Epoch 9/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0250\n",
            "Epoch 10/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0231\n",
            "Epoch 11/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0218\n",
            "Epoch 12/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0209\n",
            "Epoch 13/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0201\n",
            "Epoch 14/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0196\n",
            "Epoch 15/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 16/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 17/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 18/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 19/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 20/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 21/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 22/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 23/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 24/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 25/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 26/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 27/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 28/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 29/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 30/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 31/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 32/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 33/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 34/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 35/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 36/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 37/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 38/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 39/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 40/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 41/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 42/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 43/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0171\n",
            "Epoch 44/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 45/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 46/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 47/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 48/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 49/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0171\n",
            "Epoch 50/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 51/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 52/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 53/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 54/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 55/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 56/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 57/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 58/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 59/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 60/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 61/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 62/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 63/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 64/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 65/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 66/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 67/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 68/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 69/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 70/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 71/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 72/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 73/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 74/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 75/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 76/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 77/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 78/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 79/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0171\n",
            "Epoch 80/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 81/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 82/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 83/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 84/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 85/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 86/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 87/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 88/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 89/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0171\n",
            "Epoch 90/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 91/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 92/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 93/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 94/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 95/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 96/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 97/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 98/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 99/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 100/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 101/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 102/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 103/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 104/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 105/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 106/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 107/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 108/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 109/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 110/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 111/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 112/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 113/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 114/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 115/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 116/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 117/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 118/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 119/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 120/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 121/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 122/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0179\n",
            "Epoch 123/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 124/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 125/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 126/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 127/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 128/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 129/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 130/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 131/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 132/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 133/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 134/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 135/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 136/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0182\n",
            "Epoch 137/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 138/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0175\n",
            "Epoch 139/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0177\n",
            "Epoch 140/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 141/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 142/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0174\n",
            "Epoch 143/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 144/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 145/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0182\n",
            "Epoch 146/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 147/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 148/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 149/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 150/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 151/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 152/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 153/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 154/250\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.0174\n",
            "Epoch 155/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 156/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 157/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 158/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 159/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 160/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 161/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 162/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 163/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 164/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 165/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 166/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 167/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 168/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 169/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 170/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 171/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 172/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 173/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0190\n",
            "Epoch 174/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 175/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 176/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 177/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 178/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 179/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 180/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 181/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 182/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 183/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 184/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 185/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 186/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 187/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 188/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 189/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 190/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 191/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 192/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 193/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 194/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 195/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 196/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 197/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 198/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 199/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 200/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 201/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 202/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 203/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 204/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 205/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 206/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 207/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 208/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 209/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 210/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 211/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 212/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 213/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 214/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 215/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 216/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0178\n",
            "Epoch 217/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 218/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 219/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 220/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 221/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 222/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 223/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 224/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 225/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 226/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 227/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 228/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 229/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 230/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 231/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 232/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0172\n",
            "Epoch 233/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 234/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 235/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 236/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 237/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 238/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 239/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0177\n",
            "Epoch 240/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 241/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0175\n",
            "Epoch 242/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 243/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 244/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 245/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0193\n",
            "Epoch 246/250\n",
            "44/44 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 247/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 248/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 249/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 250/250\n",
            "44/44 [==============================] - 0s 2ms/step - loss: 0.0172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "698161f3-277c-4d43-a569-7feec4f87ed8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcf939bb8d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "b5d116c6-78cd-41b0-9c0f-e8460b4da9a6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.13063753327444497\n",
            "Using the training data mean of 0.5156161260806462 would have has resulted in a RMSE of 0.1805106219054207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/LR/All_Normalised_Whole/test_set.csv')\n",
        "test_predictors = test[columns[0:qty_predictors]]\n",
        "normalise_w_params(test_predictors, scale_params, columns[0:qty_predictors])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test[columns[qty_predictors]]\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the 5 test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "b9d5044a-2bc4-4987-d886-a2ae4b69c936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    predicted  actual  error_squared\n",
            "0  767.747864     726    1742.884129\n",
            "1  638.848633     626     165.087365\n",
            "2  715.796387     732     262.557083\n",
            "3  719.261108     735     247.712709\n",
            "4  797.616943     794      13.082279\n",
            "The RMSE on the 5 test values is 22.051410684800892.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}
