{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMT/Z71lI3pl4cVwWj6tdMy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattIzon/16010269_DataAnalytics/blob/main/DNN/Model4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN Model 4 No District Separation\n",
        "\n",
        "Predictors Removed:\n",
        "*   date\n",
        "*   gust\n",
        "*   hail\n",
        "*   tornado_funnel_cloud\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_364iE3AwpA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J9vp7tzGf3eA"
      },
      "outputs": [],
      "source": [
        "# Set-up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def normalise(df, column_list):\n",
        "  # Normalises df columns in column_list returning a dictionary of column_name: (min_value, max_value) that can be used to recover the original values\n",
        "  params = dict()\n",
        "\n",
        "  for col in column_list:\n",
        "    min = df[col].min()\n",
        "    max = df[col].max()\n",
        "    params[col] = (min, max)\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def normalise_w_params(df, params, column_list):\n",
        "  # Normalises df columns using the provided params\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] - min) / (max - min)\n",
        "\n",
        "\n",
        "def denormalise(df, params, column_list):\n",
        "  # Uses the params dictionary produced during normalisation and a list of columns to recover their original values\n",
        "  for col in column_list:\n",
        "    min = params[col][0]\n",
        "    max = params[col][1]\n",
        "    df[col] = (df[col] * (max-min)) + min\n",
        "\n",
        "\n",
        "def flat_list(nested_list):\n",
        "  return [value for sublist in nested_list for value in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generic = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/DNN/DNN_day_generic_set.csv')"
      ],
      "metadata": {
        "id": "bs_BolkcBckJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Specific Code goes here"
      ],
      "metadata": {
        "id": "vfxwSyjzIadb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm = ['crime_count', 'temp', 'dewp', 'slp', 'stp', 'visib', 'wdsp', 'mxpsd', 'max', 'min', 'prcp', 'sndp', 'year']\n",
        "data = generic.drop(['date', 'gust', 'hail', 'tornado_funnel_cloud'], axis=1)\n",
        "scale_params = normalise(data, norm)"
      ],
      "metadata": {
        "id": "y4jsaM5JzRIF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate train / eval predictors / targets\n",
        "qty_predictors = len(data.columns) - 1\n",
        "qty_targets = 1\n",
        "train_size = int(len(data)*0.8)\n",
        "\n",
        "train_predictors = data.iloc[:train_size,1:]\n",
        "train_targets = data.iloc[:train_size,0]\n",
        "\n",
        "eval_predictors = data.iloc[train_size:,1:]\n",
        "eval_targets = data.iloc[train_size:,0]"
      ],
      "metadata": {
        "id": "Z2KCqEQpiMlT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model: layers ~ inputs(58) ->  h1(30) -> h2(15) -> outputs(1)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Input(shape = (qty_predictors,)))\n",
        "model.add(tf.keras.layers.Dense(30, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(15, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.0001))"
      ],
      "metadata": {
        "id": "vjj8Be7yqkLH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJsFwGFziXv",
        "outputId": "34952d5d-d673-4bf5-8004-85b03709a2d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 30)                1080      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 15)                465       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 16        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,561\n",
            "Trainable params: 1,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(train_predictors, train_targets, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2UP_cSK0QnO",
        "outputId": "d6126c2c-19b6-464f-d721-a5103fc80ad6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0234\n",
            "Epoch 2/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0231\n",
            "Epoch 3/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0227\n",
            "Epoch 4/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0224\n",
            "Epoch 5/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0221\n",
            "Epoch 6/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0218\n",
            "Epoch 7/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0214\n",
            "Epoch 8/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0212\n",
            "Epoch 9/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0208\n",
            "Epoch 10/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0206\n",
            "Epoch 11/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0203\n",
            "Epoch 12/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0200\n",
            "Epoch 13/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0198\n",
            "Epoch 14/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0195\n",
            "Epoch 15/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0192\n",
            "Epoch 16/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0190\n",
            "Epoch 17/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 18/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 19/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0183\n",
            "Epoch 20/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 21/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 22/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.0178\n",
            "Epoch 23/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 24/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 25/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 26/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 27/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 28/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 29/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0168\n",
            "Epoch 30/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0166\n",
            "Epoch 31/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0165\n",
            "Epoch 32/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0164\n",
            "Epoch 33/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0163\n",
            "Epoch 34/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0162\n",
            "Epoch 35/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0162\n",
            "Epoch 36/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0161\n",
            "Epoch 37/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0160\n",
            "Epoch 38/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0159\n",
            "Epoch 39/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0159\n",
            "Epoch 40/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0158\n",
            "Epoch 41/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0157\n",
            "Epoch 42/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0157\n",
            "Epoch 43/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 44/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 45/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0155\n",
            "Epoch 46/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0155\n",
            "Epoch 47/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0155\n",
            "Epoch 48/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0154\n",
            "Epoch 49/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0154\n",
            "Epoch 50/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0153\n",
            "Epoch 51/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0153\n",
            "Epoch 52/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0153\n",
            "Epoch 53/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0153\n",
            "Epoch 54/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 55/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 56/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 57/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0151\n",
            "Epoch 58/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0151\n",
            "Epoch 59/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0151\n",
            "Epoch 60/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0151\n",
            "Epoch 61/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0150\n",
            "Epoch 62/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0150\n",
            "Epoch 63/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0150\n",
            "Epoch 64/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0150\n",
            "Epoch 65/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0150\n",
            "Epoch 66/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.0150\n",
            "Epoch 67/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 68/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 69/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 70/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 71/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 72/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 73/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 74/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 75/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 76/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 77/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 78/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 79/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 80/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 81/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 82/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 83/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 84/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 85/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 86/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 87/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 88/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 89/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 90/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.0147\n",
            "Epoch 91/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0147\n",
            "Epoch 92/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 93/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 94/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 95/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 96/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 97/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 98/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 99/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 100/100\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View training history\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "e9wcugdS2zyM",
        "outputId": "613888e9-d130-4830-9d1a-3c26f6acc0c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f10e23cb090>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "eval_predictions = flat_list(model.predict(eval_predictors))"
      ],
      "metadata": {
        "id": "F3hcZ9wt31pC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse model quality vs mean \n",
        "rmse = np.sqrt(np.mean((eval_targets.values - eval_predictions)**2))\n",
        "print('LinearRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "avg = np.mean(train_targets)\n",
        "\n",
        "rmse = np.sqrt(np.mean((eval_targets.values - avg)**2))\n",
        "print('Using the training data mean of {0} would have has resulted in a RMSE of {1}'.format(avg, rmse));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iYPVeVC7KYR",
        "outputId": "dbc7e9c3-d15d-4972-9ed7-65840e4ebfe7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression has RMSE of 0.12432294598872626\n",
            "Using the training data mean of 0.5124725877192983 would have has resulted in a RMSE of 0.18958985670076428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/MattIzon/16010269_DataAnalytics/main/DNN/DNN_day_test_set.csv')\n",
        "test.drop(['date', 'gust', 'hail', 'tornado_funnel_cloud'], axis=1, inplace=True)\n",
        "test_predictors = test.iloc[:,1:]\n",
        "normalise_w_params(test_predictors, scale_params, norm[1:])\n",
        "\n",
        "test_predictions = pd.DataFrame(flat_list(model.predict(test_predictors)), columns=['crime_count'])\n",
        "denormalise(test_predictions, scale_params, ['crime_count'])\n",
        "test_targets = test['crime_count']\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['predicted'] = test_predictions\n",
        "results['actual'] = test_targets\n",
        "results['error_squared'] = (results['predicted'] - results['actual']) ** 2\n",
        "print(results)\n",
        "\n",
        "print('The RMSE on the test values is {}.'.format(np.sqrt(np.mean(results.error_squared))))\n",
        "print()"
      ],
      "metadata": {
        "id": "jsSl4tvr2aNX",
        "outputId": "b15f4366-cbf5-4660-9995-603a6fe747c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     predicted  actual  error_squared\n",
            "0   642.455200     631     131.221612\n",
            "1   681.611877     686      19.255620\n",
            "2   820.900452     859    1451.575584\n",
            "3   674.215210     720    2096.246999\n",
            "4   749.884705     745      23.860339\n",
            "5   746.259766     791    2001.688572\n",
            "6   785.646973     702    6996.816035\n",
            "7   682.620117     726    1881.814233\n",
            "8   712.157837     741     831.870371\n",
            "9   626.558350     572    2976.613512\n",
            "10  740.569031     720     423.085026\n",
            "11  799.991455     826     676.444409\n",
            "12  624.695557     640     234.225987\n",
            "13  697.392883     713     243.582092\n",
            "14  685.373352     717    1000.244861\n",
            "15  768.274780     751     298.418033\n",
            "16  771.353882     707    4141.422107\n",
            "The RMSE on the test values is 38.675410481511186.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}